{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["sHHNlTB4xzwK","mgApeVdFBzBd","yrp98ro3YIvC"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WxblbQy2Ue0E","outputId":"ae7886c5-33e3-437f-b488-e0fc6c86e6aa","executionInfo":{"status":"ok","timestamp":1704463507934,"user_tz":-60,"elapsed":45994,"user":{"displayName":"Nicola Greco","userId":"11777430175931802544"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["All the code and functions (to load data and to load the model) are based on my drive organization of the files:\n","  \n","\n","```\n","  Drive:\n","      - Group Project:\n","                  - Cross:\n","                        - train\n","                        - test1...\n","                  - best_model_2 - where the model is saved\n","```\n","\n","\n","So the path to the train set is: '/content/drive/My Drive/Group Project/Cross/train' and the path to the model folder is  '/content/drive/My Drive/Group Project/best_model_2'\n","\n","\n"],"metadata":{"id":"w3MT9tVYEbET"}},{"cell_type":"markdown","metadata":{"id":"-sTC4ifUx3JH"},"source":["## LIB & FUNCTIONS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"da12927c"},"outputs":[],"source":["###### ----- FUNCTION ----- ######\n","import json\n","import os\n","import h5py\n","import numpy as np\n","import re\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import ParameterGrid\n","from tensorflow.keras.layers import Input, Conv1D, Conv2D, MaxPooling2D, Flatten, Dense, ReLU, BatchNormalization, Reshape, Softmax, Lambda, Dropout\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.regularizers import l2\n","import gc\n","from tensorflow.keras import backend as K\n","from tqdm import tqdm\n","from sklearn.preprocessing import StandardScaler\n","import sys\n","\n","def sizeof_fmt(num, suffix='B'):\n","    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n","    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n","        if abs(num) < 1024.0:\n","            return \"%3.1f %s%s\" % (num, unit, suffix)\n","        num /= 1024.0\n","    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n","\n","def get_label_from_filename(filename):\n","    # Regular expression to match the pattern '_<digits>_<digits>.h5' in the filename\n","    pattern = r'(_\\d+_\\d+\\.h5)$'\n","    # Remove the matched pattern to get the label\n","    label = re.sub(pattern, '', filename)\n","    return label\n","\n","def downsample_tensor(meg_tensor, downsampling_factor):\n","    processed_tensor = []\n","\n","    for matrix in meg_tensor:\n","        # Squeeze the single-channel dimension for processing\n","        matrix_squeezed = np.squeeze(matrix, axis=-1)\n","\n","        # Downsample the data\n","        matrix_downsampled = matrix_squeezed[:, ::downsampling_factor]\n","\n","        # Expand dimensions to get back to 3D\n","        matrix_standardized_3d = np.expand_dims(matrix_downsampled, axis=-1)\n","\n","        processed_tensor.append(matrix_standardized_3d)\n","\n","    # Convert list to numpy array\n","    return np.array(processed_tensor)\n","\n","def downsample_and_scale_tensor(meg_tensor, downsampling_factor):\n","    processed_tensor = []\n","\n","    for matrix in meg_tensor:\n","        # Squeeze the single-channel dimension for processing\n","        matrix_squeezed = np.squeeze(matrix, axis=-1)\n","\n","        # Downsample the data\n","        matrix_downsampled = matrix_squeezed[:, ::downsampling_factor]\n","\n","        # Initialize the StandardScaler\n","        scaler = StandardScaler()\n","\n","        # Time-wise scaling for each matrix\n","        matrix_standardized = scaler.fit_transform(matrix_downsampled.T).T  # Transpose data to scale along the correct axis\n","\n","        # Expand dimensions to get back to 3D\n","        matrix_standardized_3d = np.expand_dims(matrix_standardized, axis=-1)\n","\n","        processed_tensor.append(matrix_standardized_3d)\n","\n","    # Convert list to numpy array\n","    return np.array(processed_tensor)\n","\n","\n","def load_tests():\n","    ## -------  TEST DATASET 1  ------##\n","    directory = '/content/drive/My Drive/Group Project/Cross/test1'\n","    # directory = \"Cross/test1\"\n","    all_data = []\n","    labels = []\n","\n","    # Iterate through all files in the directory\n","    for filename in os.listdir(directory):\n","        if filename.endswith(\".h5\"):\n","            file_path = os.path.join(directory, filename)\n","            with h5py.File(file_path, 'r') as f:\n","                for dataset_name in f.keys():\n","                    data = f[dataset_name][()]\n","\n","                    # Add a new axis to make it a 3D tensor\n","                    data_3d = data[:, :, np.newaxis]\n","\n","                    all_data.append(data_3d)\n","                    labels.append(get_label_from_filename(filename))\n","\n","    # Convert list to numpy array\n","    test_data_1 = np.stack(all_data)\n","    test_label_1 = np.array(labels)\n","    test_data_1 = downsample_and_scale_tensor(test_data_1, 10)\n","\n","\n","    ## -------  TEST DATASET 2  ------##\n","    directory = '/content/drive/My Drive/Group Project/Cross/test2'\n","    # directory = \"Cross/test2\"\n","    all_data = []\n","    labels = []\n","\n","    # Iterate through all files in the directory\n","    for filename in os.listdir(directory):\n","        if filename.endswith(\".h5\"):\n","            file_path = os.path.join(directory, filename)\n","            with h5py.File(file_path, 'r') as f:\n","                for dataset_name in f.keys():\n","                    data = f[dataset_name][()]\n","\n","                    # Add a new axis to make it a 3D tensor\n","                    data_3d = data[:, :, np.newaxis]\n","\n","                    all_data.append(data_3d)\n","                    labels.append(get_label_from_filename(filename))\n","\n","    # Convert list to numpy array\n","    test_data_2 = np.stack(all_data)\n","    test_label_2 = np.array(labels)\n","    test_data_2 = downsample_and_scale_tensor(test_data_2, 10)\n","\n","    ## -------  TEST DATASET 3  ------##\n","    directory = '/content/drive/My Drive/Group Project/Cross/test3'\n","    # directory = \"Cross/test3\"\n","    all_data = []\n","    labels = []\n","\n","    # Iterate through all files in the directory\n","    for filename in os.listdir(directory):\n","        if filename.endswith(\".h5\"):\n","            file_path = os.path.join(directory, filename)\n","            with h5py.File(file_path, 'r') as f:\n","                for dataset_name in f.keys():\n","                    data = f[dataset_name][()]\n","\n","                    # Add a new axis to make it a 3D tensor\n","                    data_3d = data[:, :, np.newaxis]\n","\n","                    all_data.append(data_3d)\n","                    labels.append(get_label_from_filename(filename))\n","\n","    # Convert list to numpy array\n","    test_data_3 = np.stack(all_data)\n","    test_label_3 = np.array(labels)\n","    test_data_3 = downsample_and_scale_tensor(test_data_3, 10)\n","\n","    return test_data_1, test_label_1, test_data_2, test_label_2, test_data_3, test_label_3\n","\n","def get_one_hot_label(labels):\n","\n","    # Unique string labels\n","    unique_labels = np.unique(labels)\n","    num_classes = len(unique_labels)\n","\n","    # Create a mapping from string labels to integers\n","    label_to_int = {label: i for i, label in enumerate(unique_labels)}\n","\n","    # Convert string labels to integers using the mapping\n","    labels_int = np.array([label_to_int[label] for label in labels])\n","\n","\n","    # Convert integer labels to one-hot encoding\n","    labels_cat = to_categorical(labels_int, num_classes=num_classes)\n","\n","    return labels_cat\n","\n","def load_test():\n","    test_data_1, test_label_1, test_data_2, test_label_2, test_data_3, test_label_3 = load_tests()\n","    test_label_1_cat = get_one_hot_label(test_label_1)\n","    test_label_2_cat = get_one_hot_label(test_label_2)\n","    test_label_3_cat = get_one_hot_label(test_label_3)\n","    test_data = tf.concat([test_data_1, test_data_2,  test_data_3], axis=0)\n","    test_label_cat = tf.concat([test_label_1_cat, test_label_2_cat, test_label_3_cat], axis=0)\n","    test_label = tf.concat([test_label_1, test_label_2, test_label_3], axis=0)\n","    return test_data, test_label_cat\n","\n","def load_datasets():\n","\n","    ## -----  TRAIN DATASET  ----- ##\n","    directory = '/content/drive/My Drive/Group Project/Cross/train'\n","    # directory = \"Cross/train\"\n","    all_data = []\n","    labels = []\n","\n","    # Iterate through all files in the directory\n","    # Iterate through all files in the directory\n","    for filename in os.listdir(directory):\n","        if filename.endswith(\".h5\"):\n","            file_path = os.path.join(directory, filename)\n","            with h5py.File(file_path, 'r') as f:\n","                for dataset_name in f.keys():\n","                    data = f[dataset_name][()]\n","\n","                    # Add a new axis to make it a 3D tensor\n","                    data_3d = data[:, :, np.newaxis]\n","\n","                    all_data.append(data_3d)\n","                    labels.append(get_label_from_filename(filename))\n","\n","    # Convert list to numpy array\n","    train_data = np.stack(all_data)\n","    train_label = np.array(labels)\n","    train_data = downsample_and_scale_tensor(train_data, 10)\n","    train_label_cat = get_one_hot_label(train_label)\n","\n","    print(\"Data Train shape:\", train_data.shape, \" - size: \", sizeof_fmt(sys.getsizeof(train_data)))\n","    print(\"Labels Train one hot shape:\", train_label_cat.shape)\n","\n","\n","    test_data, test_label_cat = load_test()\n","    test_data = test_data.numpy()\n","\n","    print(\"Data Test shape:\", test_data.shape, \" - size: \", sizeof_fmt(sys.getsizeof(test_data)))\n","    print(\"Labels Test one hot shape:\", test_label_cat.shape)\n","\n","    test_data = tf.convert_to_tensor(test_data)\n","    train_data = tf.convert_to_tensor(train_data)\n","    train_label_cat = tf.convert_to_tensor(train_label_cat)\n","\n","    return train_data, train_label_cat, test_data, test_label_cat\n","\n"]},{"cell_type":"code","source":["def load_model(path):\n","    best_model = tf.keras.models.load_model(path)\n","    with open(path + '/hyperparameters.json', 'r') as file:\n","        best_params = json.load(file)\n","    print(\"Hyperparameters:\", best_params)\n","    best_model.summary()\n","    return best_model, best_params\n","\n","def accuracy_on_tests(best_model, scale):\n","  test_data_1, test_label_1, test_data_2, test_label_2, test_data_3, test_label_3 = load_tests()\n","  test_label_1_cat = get_one_hot_label(test_label_1)\n","  test_label_2_cat = get_one_hot_label(test_label_2)\n","  test_label_3_cat = get_one_hot_label(test_label_3)\n","  test_data_1 = downsample_tensor(test_data_1, scale)\n","  test_data_2 = downsample_tensor(test_data_2, scale)\n","  test_data_3 = downsample_tensor(test_data_3, scale)\n","  print(\"Model validation on Cross/test1: \")\n","    # Evaluate the model on the validation set\n","  val_loss_1, val_accuracy_1 = best_model.evaluate(test_data_1, test_label_1_cat)\n","  print(\"Validation Loss:\", val_loss_1)\n","  print(\"Validation Accuracy:\", val_accuracy_1)\n","\n","  print(\"------------------------------------------\")\n","  print(\"Model validation on Cross/test2: \")\n","    # Evaluate the model on the validation set\n","  val_loss_2, val_accuracy_2 = best_model.evaluate(test_data_2, test_label_2_cat)\n","  print(\"Validation Loss:\", val_loss_2)\n","  print(\"Validation Accuracy:\", val_accuracy_2)\n","\n","  print(\"------------------------------------------\")\n","  print(\"Model validation on Cross/test3: \")\n","    # Evaluate the model on the validation set\n","  val_loss_3, val_accuracy_3 = best_model.evaluate(test_data_3, test_label_3_cat)\n","  print(\"Validation Loss:\", val_loss_3)\n","  print(\"Validation Accuracy:\", val_accuracy_3)\n","\n","  print(\"------------------------------------------\")\n","\n","def accuracy_on_test(best_model, scale):\n","  test_data, test_label_cat = load_test()\n","  test_data = downsample_tensor(test_data, scale)\n","  print(\"Model validation on Cross/test1+2+3: \")\n","    # Evaluate the model on the validation set\n","  val_loss_1, val_accuracy_1 = best_model.evaluate(test_data, test_label_cat)\n","  print(\"Validation Loss:\", val_loss_1)\n","  print(\"Validation Accuracy:\", val_accuracy_1)\n","\n","  print(\"------------------------------------------\")\n","\n","\n"],"metadata":{"id":"GFJ3Ciu9XGAh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sHHNlTB4xzwK"},"source":["## DATA LOAD"]},{"cell_type":"markdown","source":["##### There is no need to run this codes - Go directly to Model Load and Test\n","\n"],"metadata":{"id":"mgApeVdFBzBd"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"0ebf0aba","executionInfo":{"status":"error","timestamp":1704443755749,"user_tz":-60,"elapsed":8449,"user":{"displayName":"Nicola Greco","userId":"11777430175931802544"}},"outputId":"0b351f7c-5fb2-4b28-c4a7-2f527fe76559"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-1ede3dc1954a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-9ec32b25905b>\u001b[0m in \u001b[0;36mload_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0;31m# Add a new axis to make it a 3D tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fast_read_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fast_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Fall back to Python read pathway below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# To load the train and test datasets with labels in one-hot encoding structure\n","# the test dataset is formed by the 3 test set in cross folder concatenated\n","# the load_datasets() function already load the dataset downscaled by a factor 10 and scaled\n","train_data, train_label_cat, test_data, test_label_cat = load_datasets()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ff71ea2"},"outputs":[],"source":["# The function load_tests() load the 3 test sets individually\n","# The test set are downsamplet by a factor 10 and the lable are not in one-hot encoding\n","test_data_1, test_label_1, test_data_2, test_label_2, test_data_3, test_label_3 = load_tests()"]},{"cell_type":"code","source":["# Same as before just to load the merged test set with one-hot encoding labels\n","test_data, test_label_cat = load_test()"],"metadata":{"id":"t_58eEwtWJzl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" ## MODEL LOAD\n"],"metadata":{"id":"JTWj1LxrWb2E"}},{"cell_type":"code","source":["best_model_path = '/content/drive/My Drive/Group Project/best_model_2'\n","best_model, best_params = load_model(best_model_path)"],"metadata":{"id":"dFluLo7GWfAx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704466059230,"user_tz":-60,"elapsed":988,"user":{"displayName":"Nicola Greco","userId":"11777430175931802544"}},"outputId":"d46a4a44-010c-4f77-91ec-a900c3f9e7ee"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Hyperparameters: {'k': 3, 'l': 6, 'p': 1, 's': 1, 'dropout_rate': 0.8, 'l2_lambda': 0.001}\n","Model: \"model_1408\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1409 (InputLayer)     [(None, 248, 594, 1)]     0         \n","                                                                 \n"," conv2d_1408 (Conv2D)        (None, 1, 589, 3)         4467      \n","                                                                 \n"," lambda_1408 (Lambda)        (None, 3, 589, 1)         0         \n","                                                                 \n"," max_pooling2d_1408 (MaxPoo  (None, 3, 196, 1)         0         \n"," ling2D)                                                         \n","                                                                 \n"," flatten_1408 (Flatten)      (None, 588)               0         \n","                                                                 \n"," dropout_1408 (Dropout)      (None, 588)               0         \n","                                                                 \n"," dense_1408 (Dense)          (None, 4)                 2356      \n","                                                                 \n","=================================================================\n","Total params: 6823 (26.65 KB)\n","Trainable params: 6823 (26.65 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["## MODEL TEST\n"],"metadata":{"id":"yrp98ro3YIvC"}},{"cell_type":"code","source":["# the second argument is the downsampling factor\n","# Consider that the function that the data are loaded downsample by a factor 10, so with size: 248x3563x1\n","# So if the model has input size 248x594x1 you need a further downsampling by a factor 6 to make the data have the same size of the model\n","# The dataset are loaded within the function accuracy_on_tests() so it may take a while to run.\n","accuracy_on_tests(best_model, 6)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LCJEwGC3bBby","outputId":"97e2079a-aea8-43d7-ac6d-fcc0c0760e9d","executionInfo":{"status":"ok","timestamp":1704443932659,"user_tz":-60,"elapsed":118604,"user":{"displayName":"Nicola Greco","userId":"11777430175931802544"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model validation on Cross/test1: \n","1/1 [==============================] - 1s 639ms/step - loss: 0.8622 - accuracy: 0.6875\n","Validation Loss: 0.8621693253517151\n","Validation Accuracy: 0.6875\n","------------------------------------------\n","Model validation on Cross/test2: \n","1/1 [==============================] - 0s 104ms/step - loss: 1.0154 - accuracy: 0.6875\n","Validation Loss: 1.015397548675537\n","Validation Accuracy: 0.6875\n","------------------------------------------\n","Model validation on Cross/test3: \n","1/1 [==============================] - 0s 111ms/step - loss: 0.8194 - accuracy: 0.7500\n","Validation Loss: 0.8193597197532654\n","Validation Accuracy: 0.75\n","------------------------------------------\n"]}]},{"cell_type":"code","source":["accuracy_on_test(best_model, 6)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vXw7agUudNvz","outputId":"b6ad641c-60ab-41c7-c956-62f9a242dce8","executionInfo":{"status":"ok","timestamp":1704443953932,"user_tz":-60,"elapsed":21289,"user":{"displayName":"Nicola Greco","userId":"11777430175931802544"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model validation on Cross/test1+2+3: \n","2/2 [==============================] - 0s 41ms/step - loss: 0.8990 - accuracy: 0.7083\n","Validation Loss: 0.8989755511283875\n","Validation Accuracy: 0.7083333134651184\n","------------------------------------------\n"]}]},{"cell_type":"code","source":["best_model.layers[2].get_config()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LRs_VuQVyW_j","executionInfo":{"status":"ok","timestamp":1704465221635,"user_tz":-60,"elapsed":421,"user":{"displayName":"Nicola Greco","userId":"11777430175931802544"}},"outputId":"311ab872-8b99-4396-9796-3ded99f2061b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'name': 'lambda_1408',\n"," 'trainable': True,\n"," 'dtype': 'float32',\n"," 'function': ('4wEAAAAAAAAAAAAAAAUAAAATAAAA8zwAAACXAHQAAAAAAAAAAAAAAKABAAAAAAAAAAAAAAAAAAAA\\nAAAAAAB8AGcAZAGiAaYCAACrAgAAAAAAAAAAUwApAk4pBOkAAAAA6QMAAADpAgAAAOkBAAAAKQLa\\nAnRm2gl0cmFuc3Bvc2UpAdoBeHMBAAAAIPo/QzovVXNlcnMvbmljY28vQXBwRGF0YS9Mb2NhbC9U\\nZW1wL2lweWtlcm5lbF8xNDI5Ni8zMjk1NDc4MDE4LnB5+gg8bGFtYmRhPvohYnVpbGRfY25uX21v\\nZGVsLjxsb2NhbHM+LjxsYW1iZGE+CgAAAHMYAAAAgAClEqccohyoYbAcsBywHNEhPtQhPoAA8wAA\\nAAA=\\n',\n","  None,\n","  None),\n"," 'function_type': 'lambda',\n"," 'module': '__main__',\n"," 'output_shape': None,\n"," 'output_shape_type': 'raw',\n"," 'output_shape_module': None,\n"," 'arguments': {}}"]},"metadata":{},"execution_count":24}]}]}